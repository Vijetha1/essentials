import numpy as np
np.random.seed(42)
import random as rn
rn.seed(12345)

import pdb
from scipy import io
import h5py
# from skimage.transform import resize
from scipy.misc import imresize as resize
# import sys
# sys.path.insert(0, '/media/vrpg/parent/vijetha/CVPR_2018_multiHashing')


def convertLevelDbtoHdf5(sourcePath, targetPath):
	"""
	Description:
		converts a level-db file (generated by caffe) to a hdf5 file.
	Args:
		sourcePath - file path at which the level-db file is present.
		targetPath - file path to which the hdf5 file is to be stored.
	Outputs:
		stores a hdf5file at the give path 'sourcePath'
	"""
	import caffe
	import leveldb
	from caffe.proto import caffe_pb2
	dbData = leveldb.LevelDB(sourcePath)
	datum = caffe_pb2.Datum()

	data = []
	for key, value in dbData.RangeIter():
	    datum.ParseFromString(value)
	    data.append(caffe.io.datum_to_array(datum))
	data = np.array(data)
	# pdb.set_trace()
	data = np.reshape(data, (data.shape[0], data.shape[1]))
	f = h5py.File(targetPath, 'w')
	f.create_dataset('data', data=data)
	f.close()


def mergeHdf5Files(listOfFilesToMerge, nameOfNewFile):
	"""
	Desc: 

	Args:

	Outputs:

	"""
	fNew = h5py.File(nameOfNewFile, 'w')
	for i in range(len(listOfFilesToMerge)):
		fOld = h5py.File(listOfFilesToMerge[i][0], 'r')
		data = fOld['data'][:]
		fOld.close()
		fNew.create_dataset(listOfFilesToMerge[i][1], data=data)
		del data
	fNew.close()

def getRealValuedToCode(x, threshold):
	"""
	Desc:

	Args:

	Outputs:

	"""
	x = np.array(x>threshold, dtype='int32')
	return x

def oneHotVectors(x, n):
	"""
	Desc:

	Args:

	Outputs:

	"""
	# pdb.set_trace()
	if np.max(x) == 0:
		raise AssertionError()
	m = max(x.shape)
	# n = np.max(x)
	y = np.zeros((m, n))
	x = np.reshape(x, (m, ))
	y[np.arange(m), x] = 1
	return y

def computemAP(hammingRank, groundTruthSimilarity, trackPrec = False):
	"""
	Desc:

	Args:

	Outputs:

	"""
	[Q, N] = hammingRank.shape
	pos = np.arange(N)+1
	MAP = 0
	numSucc = 0
	if trackPrec:
		rareClsVsPrec = []
	for i in range(Q):
		ngb = groundTruthSimilarity[i, np.asarray(hammingRank[i,:], dtype='int32')]
		nRel = np.sum(ngb)
		if nRel > 0:
			prec = np.divide(np.cumsum(ngb), pos)
			#prec = prec[0:5000]
			# pdb.set_trace()
			#ap = np.mean(prec[np.asarray(ngb[0:5000], dtype='bool')])
			prec = prec
			ap = np.mean(prec[np.asarray(ngb, dtype='bool')])
			MAP = MAP + ap
			numSucc = numSucc + 1
			if trackPrec:
				rareClsVsPrec.append((nRel, ap))
	MAP = float(MAP)/numSucc
	if trackPrec:
		import scipy.io as sio 
		sio.savemat('rareClsVsPrec.mat',{'rareClsVsPrec':rareClsVsPrec})
	return MAP


def computeSimilarity(queryLabels, databaseLabels, typeOfData='singleLabelled'):
	"""
	Desc:

	Args:

	Outputs:

	"""
	groundTruthSimilarityMatrix = np.zeros((queryLabels.shape[0], databaseLabels.shape[0]))
	if typeOfData=='singleLabelled':
		queryLabels = np.reshape(queryLabels, (max(queryLabels.shape),))
		databaseLabels = np.reshape(databaseLabels, (max(databaseLabels.shape),))
		for i in range(queryLabels.shape[0]):
			groundTruthSimilarityMatrix[i,:] = queryLabels[i] == databaseLabels
	elif typeOfData=='multiLabelled':
		for i in range(queryLabels.shape[0]):
			curQue = queryLabels[i][:]
			if sum(curQue) != 0:
				threshold = 1
				sim = np.sum(np.logical_and(curQue, databaseLabels), axis=-1)
				den = np.sum(np.logical_or(curQue, databaseLabels), axis=-1)
				groundTruthSimilarityMatrix[i][np.where(sim >= threshold)[0]] = 1
	groundTruthSimilarityMatrix = np.asarray(groundTruthSimilarityMatrix, dtype='float32')
	return groundTruthSimilarityMatrix


def calcHammingRank(queryHashes, databaseHashes):
	"""
	Desc:

	Args:

	Outputs:

	"""
	hammingDist = np.zeros((queryHashes.shape[0], databaseHashes.shape[0]))
	hammingRank = np.zeros((queryHashes.shape[0], databaseHashes.shape[0]))
	for i in range(queryHashes.shape[0]):
		hammingDist[i] = np.reshape(np.sum(np.abs(queryHashes[i] - databaseHashes), axis=1), (databaseHashes.shape[0], ))
		hammingRank[i] = np.argsort(hammingDist[i])
	return hammingDist, hammingRank


def prAtK(hammingDist, groundTruthSimilarity, k):
	"""
	Desc:

	Args:

	Outputs:

	"""
	countOrNot = np.array(hammingDist <= k, dtype='int32')
	newSim = np.multiply(groundTruthSimilarity, countOrNot)
	countOrNot = countOrNot + 0.000001
	#pdb.set_trace()
	prec = np.mean(np.divide(np.sum(newSim, axis=-1), np.sum(countOrNot, axis=-1)))
	rec = np.mean(np.divide(np.sum(newSim, axis=-1), np.sum(groundTruthSimilarity, axis=-1)))
	return (prec, rec)


def writeHashingResultsToCsv(results, fileName, mode, approaches, datasets, nBits, toCompute):
	"""
	Desc:

	Args:

	Outputs:

	"""
	import csv
	with open(fileName, mode) as csvfile:
		mywriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)
		for z in range(len(toCompute)):
			mywriter.writerow([toCompute[z]])
			row = ['Approaches']
			for ii in range(len(datasets)):
				for jj in range((len(nBits))):
					if jj == 0:
						row.append(datasets[ii])
					else:
						row.append('-')
			mywriter.writerow(row)
			row = [' ']
			for i in range(len(datasets)):
				for j in range((len(nBits))):
					row.append(nBits[j])
			mywriter.writerow(row)
			for y in range(len(approaches)):
				row = []
				row.append(approaches[y])
				for x in range(len(datasets)):
					for w in range(len(nBits)):
						num = round(results[y, x, w, z], 4)
						if num != -100:
							row.append(str(num))
						else:
							row.append('-')
				mywriter.writerow(row)
			mywriter.writerow([' '])

def numUniqueHashes(x):
	"""
	Desc:

	Args:

	Returns:

	"""
	y = np.unique(x, axis=0)
	return y.shape[0]

def getShannonEntropy(x):
	"""
	Desc:

	Args:

	Returns:

	"""
	raise NotImplementedError

def getAvgHashHistogram(hammingDist, nBits=12):
	"""
	Desc:

	Args:

	Returns:

	"""
	finalHist = np.zeros((nBits,))
	for i in range(hammingDist.shape[0]):
		# pdb.set_trace()
		finalHist = finalHist + np.histogram(hammingDist[i,:], nBits)[0]
	return finalHist

def getCosineSimilarity(x, batchSize=50, save=True, getFullMatrix=False):
	from scipy.spatial.distance import cdist
	distances = np.zeros((x.shape[0], x.shape[0]), dtype='float32')
	for i in range(int(x.shape[0]/batchSize)):
		# print(i)
		for j in range(int(x.shape[0]/batchSize)):
			if np.sum(distances[j*batchSize:(j+1)*batchSize, i*batchSize:(i+1)*batchSize]) == 0:
				dst = cdist(x[i*batchSize:(i+1)*batchSize] , x[j*batchSize:(j+1)*batchSize] ,  'cosine')
				distances[i*batchSize:(i+1)*batchSize, j*batchSize:(j+1)*batchSize] = dst
			elif getFullMatrix:
				distances[i*batchSize:(i+1)*batchSize, j*batchSize:(j+1)*batchSize] = distances[j*batchSize:(j+1)*batchSize, i*batchSize:(i+1)*batchSize]
	# pdb.set_trace()
	return distances

def getWeightShapesFromModel(model, library='Keras'):
	"""
	Desc:

	Args:

	Returns:


	"""
	# pdb.set_trace()
	weightShapes=[]
	print("Printing From Model")
	if library == 'Keras':
		nLayers = len(model.layers)
		for i in range(nLayers):
			nParamSets = len(model.layers[i].get_weights())
			assert nParamSets%2 == 0
			for j in range(int(nParamSets/2)):
				weightShapes.append([model.layers[i].get_weights()[2*j].shape, model.layers[i].get_weights()[2*j+1].shape])
				print(weightShapes[-1])
	return weightShapes


def getWeightShapesFromH5(fileName):
	"""
	Desc:

	Args:

	Returns:

	"""
	f = h5py.File(fileName, 'r')
	allKeys=[k for k in f.keys()]
	weightShapes=[]
	print("Printing From saved weights")
	for layerNumber in range(len(allKeys)):
		subKeys = [k for k in f[allKeys[layerNumber]].keys()]
		assert len(subKeys)%2 == 0
		for i in range(int(len(subKeys)/2)):
			weightShapes.append([f[allKeys[layerNumber]][subKeys[2*i]][:].shape, f[allKeys[layerNumber]][subKeys[2*i+1]][:].shape])
			print(weightShapes[-1])
	f.close()
	return weightShapes

def getWeightShapesFromNpyFile(fileName):
	"""
	"""
	weights = np.load(fileName, encoding = 'bytes').item()
	weightKeys = [key for key in weights.keys() if '__' not in key]
	for i in range(len(weightKeys)):
		curDs = weights[weightKeys[i]]
		for j in range(len(curDs)):
			print(curDs[j].shape)

def convertThtoTf(srcFileName, dstFileName):
	"""
	Desc:

	Args:

	Returns:

	"""
	from keras.utils.conv_utils import convert_kernel
	from shutil import copyfile
	copyfile(srcFileName, dstFileName)
	f = h5py.File(dstFileName, 'r+')
	allKeys=[k for k in f.keys()]
	for layerNumber in range(len(allKeys)):
		if 'conv' in allKeys[layerNumber]:
			subKeys = [k for k in f[allKeys[layerNumber]].keys()]
			for i in range(len(subKeys)):
				original_w = f[allKeys[layerNumber]][subKeys[i]][:]
				if i == 0:
					abcd = f[allKeys[layerNumber]][subKeys[0]]
					del f[allKeys[layerNumber]][subKeys[0]]
					pdb.set_trace()
					original_w = np.transpose(convert_kernel(original_w), (2, 3, 1, 0))
					data = f[allKeys[layerNumber]].create_dataset(subKeys[i], original_w.shape)
					data = original_w
	f.close()


def matToHdf5(srcFileName, dstFileName):
	"""
	"""
	# pdb.set_trace()
	data = io.loadmat(srcFileName)
	datasets = [key for key in data.keys() if '__' not in key]
	f = h5py.File(dstFileName, 'w')
	for i in range(len(datasets)):
		if datasets[i] == 'B_trn':
			f.create_dataset('train_hashes', data=data[datasets[i]])
		elif datasets[i] == 'B_tst':
			f.create_dataset('test_hashes', data=data[datasets[i]])
		elif datasets[i] == 'WtrueTestTraining':
			f.create_dataset('groundTruthSimilarity', data=data[datasets[i]])
		else:
			f.create_dataset(datasets[i], data=data[datasets[i]])
	f.close()

def prepImageData(images, chOrder='channelsLast', resizeHeight=256, resizeWidth=256, meanSubtractOrder='BGR', bsAxis = 0):
	"""
	Desc:

	Args:

	Returns:
	"""
	# pdb.set_trace()
	images = batchSizeFirst(images, bsAxis)
	if chOrder == 'channelsLast':
		images = channelsFirstToLast(images)
	elif chOrder == 'channelsFirst':
		images = channelsLastToFirst(images)
	images = resizeImages(images, resizeHeight, resizeWidth)
	images = meanSubtract(images, meanSubtractOrder)
	return images

def im2Arr(path, chOrder='channelsLast', resizeHeight=256, resizeWidth=256, meanSubtractOrder='BGR'):
	# pdb.set_trace()
	import cv2
	try:
		image = cv2.imread(path)
		image = np.expand_dims(image, axis=0)
		image = prepImageData(image, chOrder='channelsLast', resizeHeight=256, resizeWidth=256, meanSubtractOrder='BGR')
		return image
	except:
		# print("skipped")
		return None

def prepLabelData(labels, sourceType='uint', targetType='uint'):
	"""
	Desc:

	Args:

	Returns:
	
	"""
	batchSize = max(labels.shape)
	if sourceType== 'uint' and targetType == 'uint':
		labels = np.reshape(labels, (batchSize, ))
	elif sourceType == 'oneHot' and targetType == 'uint':
		labels = np.array(np.argmax(labels, axis=-1), dtype='int32')
	else:
		raise NotImplementedError
	return labels

def makeCIFAR10(srcFileName, dstFileName, printInfo = True, batchSize = 1000, chOrder='channelsLast', resizeHeight=256, resizeWidth=256, meanSubtractOrder='BGR', labelSourceType='uint', labelTargetType='uint'):
	"""
	"""
	# pdb.set_trace()
	fSrc = h5py.File(srcFileName, 'r')
	fDst = h5py.File(dstFileName, 'w')
	datasets = [key for key in fSrc.keys() if '__' not in key]
	for i in range(len(datasets)):
		dShape =  fSrc[datasets[i]][:].shape
		bs = np.max(dShape)
		bsOrder = np.argmax(dShape)
		print("Processing Dataset -"+str(datasets[i]))
		print("Total Samples - "+str(bs))
		if len(dShape) == 4 and np.max(fSrc[datasets[i]][:]) > 200:
			# pdb.set_trace()
			if chOrder == 'channelsFirst':
				fDst.create_dataset(datasets[i], (bs, 3, resizeHeight, resizeWidth))
			elif chOrder == 'channelsLast':
				fDst.create_dataset(datasets[i], (bs, resizeHeight, resizeWidth, 3))
			nBatches = int(bs/batchSize)
			for j in range(nBatches):
				if printInfo:
					print("# processed images - "+str(j*batchSize))
				if bsOrder == 3:
					image = fSrc[datasets[i]][:,:,:,j*batchSize:(j+1)*batchSize]
				elif bsOrder == 0:
					image = fSrc[datasets[i]][j*batchSize:(j+1)*batchSize,:,:,:]
				if len(image.shape) != 4:
					image = np.expand_dims(image, axis=bsOrder)
				data = prepImageData(image, chOrder, resizeHeight, resizeWidth, meanSubtractOrder, bsOrder)
				fDst[datasets[i]][j*batchSize:(j+1)*batchSize, ...] = data  # Assuming batch size always at the first dimension
		else:
			data = prepLabelData(fSrc[datasets[i]][:], labelSourceType, labelTargetType)
			fDst.create_dataset(datasets[i], data=data)
	fSrc.close()
	fDst.close()

def cleanH5(srcFile, dstFile, chOrder = 'channelsLast', batchSize=1000):
	"""
	"""
	pdb.set_trace()
	fSrc = h5py.File(srcFile, 'r')
	fDst = h5py.File(dstFile, 'w')
	datasets = [key for key in fSrc.keys() if '__' not in key]
	for i in range(len(datasets)):
		print("At dataset "+str(datasets[i]))
		ind = datasets[i].find('_')
		if 'img' in datasets[i]:
			newName = datasets[i][0:ind+1]+'data'
			bs = fSrc[datasets[i]][:].shape[0]
			fDst.create_dataset(newName, (bs, 256, 256, 3))
			for j in range(int(bs/batchSize)+1):
				if j%5 == 0:
					print("At "+str(j))
				data = fSrc[datasets[i]][j*batchSize:(j+1)*batchSize]
				data = np.transpose(data, (0, 2, 3, 1))
				fDst[newName][j*batchSize:(j+1)*batchSize, ...] = data
			# del data
		elif 'label' in datasets[i]:
			newName = datasets[i][0:ind+1]+'labels'
			data = fSrc[datasets[i]][:]
			fDst.create_dataset(newName, data=data)
			del data
		elif 'vector' in datasets[i]:
			newName = datasets[i][0:ind+1]+'vectors'
			data = fSrc[datasets[i]][:]
			fDst.create_dataset(newName, data=data)
			del data
	fDst.close()
	fSrc.close()


def makeNUS(srcFileName, dstFileName, printInfo = True, batchSize = 1000, chOrder='channelsLast', resizeHeight=256, resizeWidth=256, meanSubtractOrder='BGR', labelSourceType='uint', labelTargetType='uint'):
	"""
	"""
	# pdb.set_trace()
	fSrc = h5py.File(srcFileName, 'r')
	fDst = h5py.File(dstFileName, 'w')
	datasets = [key for key in fSrc.keys() if '__' not in key]
	for i in range(len(datasets)):
		dShape =  fSrc[datasets[i]][:].shape
		bs = np.max(dShape)
		bsOrder = np.argmax(dShape)
		print("Processing Dataset -"+str(datasets[i]))
		print("Total Samples - "+str(bs))
		if len(dShape) == 4 and np.max(fSrc[datasets[i]][:]) > 200:
			# pdb.set_trace()
			if chOrder == 'channelsFirst':
				fDst.create_dataset(datasets[i], (bs, 3, resizeHeight, resizeWidth))
			elif chOrder == 'channelsLast':
				fDst.create_dataset(datasets[i], (bs, resizeHeight, resizeWidth, 3))
			nBatches = int(bs/batchSize)
			for j in range(nBatches):
				if printInfo:
					print("# processed images - "+str(j*batchSize))
				if bsOrder == 3:
					image = fSrc[datasets[i]][:,:,:,j*batchSize:(j+1)*batchSize]
				elif bsOrder == 0:
					image = fSrc[datasets[i]][j*batchSize:(j+1)*batchSize,:,:,:]
				if len(image.shape) != 4:
					image = np.expand_dims(image, axis=bsOrder)
				data = prepImageData(image, chOrder, resizeHeight, resizeWidth, meanSubtractOrder)
				fDst[datasets[i]][j*batchSize:(j+1)*batchSize, ...] = data  # Assuming batch size always at the first dimension
		else:
			data = prepLabelData(fSrc[datasets[i]][:], labelSourceType, labelTargetType)
			fDst.create_dataset(datasets[i], data=data)
	fSrc.close()
	fDst.close()
	
def getTagMatrix(excelFileName):
	raise NotImplementedError

def resizeImages(images, resizeHeight=256, resizeWidth = 256):
	"""
	Desc:

	Args:

	Returns:
	"""
	# pdb.set_trace()
	order = images.shape
	ch = order.index(3)
	if ch == 1:
		images = channelsFirstToLast(images)
	resizedImages = np.zeros((images.shape[0], resizeHeight, resizeWidth, 3))
	for i in range(resizedImages.shape[0]):
		resizedImages[i,:,:,:] = resize(images[i], (resizeHeight, resizeWidth))
	# pdb.set_trace()
	if np.max(resizedImages) <= 1:
		resizedImages = 255.0*resizedImages
	if ch == 1:
		resizedImages = channelsLastToFirst(resizedImages)
	return resizedImages

def batchSizeFirst(images, bs=0):
	"""
	Desc:

	Args:

	Returns:
	"""
	order = images.shape
	# bs = np.argmax(order)
	assert len(order) == 4
	if bs == 3:
		images = np.transpose(images, (3, 0, 1, 2))
	elif bs == 0:
		pass
	else:
		raise NotImplementedError
	return images

def cropImages(images, cropHeight=227, cropWidth=227):
	"""
	Desc:

	Args:

	Returns:

	"""
	order = images.shape
	ch = order.index(3)
	if ch == 1:
		images = channelsFirstToLast(images)
	croppedImages = np.zeros((images.shape[0], cropHeight, cropWidth, 3))
	for i in range(croppedImages.shape[0]):
		randX = np.random.randint(images.shape[1]-cropHeight)
		randY = np.random.randint(images.shape[2]-cropWidth)
		croppedImages[i,:,:,:] = images[i,randX:randX+cropHeight,randY:randY+cropWidth,:]
	if ch == 1:
		croppedImages = channelsLastToFirst(croppedImages)
	return croppedImages

def channelsFirstToLast(images):
	"""
	Desc:

	Args:

	Returns:

	"""
	order = images.shape
	ch = order.index(3)
	if ch == 1:
		images = np.transpose(images, (0, 2, 3, 1))
	return images

def channelsLastToFirst(images):
	"""
	Desc:

	Args:

	Returns:

	"""
	order = images.shape
	ch = order.index(3)
	if ch == 3:
		images = np.transpose(images, (0, 3, 1, 2))
	return images

def meanSubtract(images, sourceDataSet='IMAGENET', order='RGB'):
	"""
	Desc:

	Args:

	Returns:

	"""
	chOrder = images.shape
	ch = chOrder.index(3)	
	if ch == 1:
		images = channelsFirstToLast(images)
	if order == 'RGB':
		#in RGB order
	    images[:, :, :, 0] -= 123.68
	    images[:, :, :, 1] -= 116.779
	    images[:, :, :, 2] -= 103.939 # values copied from https://github.com/heuritech/convnets-keras/blob/master/convnetskeras/convnets.py
	elif order == 'BGR':
	    images[:, :, :, 0] -= 103.939
	    images[:, :, :, 1] -= 116.779
	    images[:, :, :, 2] -= 123.68 # values copied from https://github.com/heuritech/convnets-keras/blob/master/convnetskeras/convnets.py
	if ch == 1:
		images = channelsLastToFirst(images)
	return images


def shuffleInUnison(images, labels):
	"""
	Desc:

	Args:

	Returns:

	"""
	perm = np.random.permutation(images.shape[0])
	images = images[perm]
	labels = labels[perm]
	return images, labels

def emailSender(mystr, sendEmail=False):
	"""
	Desc:

	Args:

	Returns:

	"""
	if sendEmail:
		import smtplib
		fromaddr = '****'
		toaddrs  = '****'
		SUBJECT = "From Python Program"
		message = """\
		From: %s
		To: %s
		Subject: %s

		%s
		""" % (fromaddr, ", ".join(toaddrs), SUBJECT, mystr)
		username = '****'
		password = '****'
		server = smtplib.SMTP('smtp.gmail.com:587')
		server.starttls()
		server.login(username,password)
		server.sendmail(fromaddr, toaddrs, message)
		server.quit()

def checkIfWeightsAreNotLost(model_1, model_2, layerList):
	"""
	Desc:

	Args:

	Returns:

	"""
	for i in range(len(layerList)):
		sameWeights = False
		weights1 = model_1.layers[layerList[i]].get_weights()[0]
		weights2 = model_2.layers[layerList[i]].get_weights()[0]
		if weights1.shape != weights2.shape:
			print("Weights Shapes did not match")
			break
		else:
			totalNumberOfWeights = getTotalWeights(weights1.shape)
			if np.sum(weights1 == weights2) != totalNumberOfWeights:
				print("Weights are different")
				break
			else:
				sameWeights = True
	return sameWeights

def getTotalWeights(weightsShape):
	"""
	Desc:

	Args:

	Returns:

	"""
	totalWeights = 1
	for i in range(len(weightsShape)):
		totalWeights = totalWeights*weightsShape[i]
	return totalWeights

def computeAccuracy(predictions, groundTruths):
	"""
	"""
	# pdb.set_trace()
	predictions = prepLabelData(predictions, sourceType='oneHot', targetType='uint')
	groundTruths = prepLabelData(groundTruths, sourceType='uint', targetType='uint')
	acc = np.sum((predictions == groundTruths))*100/predictions.shape[0]
	return acc

def loadJsonFile(fileName):
	import json
	f = open(fileName, 'r')
	# pdb.set_trace()
	allData = json.load(f)
	f.close()
	return allData

def getTagVectorsForEachImage(data, imId=None, ind=None):
	# pdb.set_trace()
	if imId != None:
		ind = [i for i,x in enumerate(data) if x[0] == imId]
	elif ind != None:
		ind = ind
	else:
		ind = np.random.randint(len(data))
	vecs = data[ind][2]
	vecs = np.asarray(vecs)
	return vecs

def getSvd(S):
	import numpy.linalg as la
	try:
		u, e, v = la.svd(S, full_matrices=True)
	except:
		# import math
		# pdb.set_trace()
		# if np.isnan(S):
		# print("True")
		u=0
		e=0
		v=0
	return u, e, v

def makeTrainTestSplits(imageIds, labels, labelType = 'oneHot', nImagesPerClassTrain=500, nImagesPerClassTest = 100):
	if labelType == 'oneHot':
		nClasses = labels.shape[1]
	#nTotalImages = nImagesPerClassTrain + nImagesPerClassTest
	#totalImages = np.zeros((nClasses, nTotalImages), dtype='uint32')
	#totallabels = np.zeros((nClasses, nTotalImages, nClasses))
	nTrainImages = int(imageIds.shape[0]*0.7)
	trainImages = imageIds[0:nTrainImages]
	testImages = imageIds[nTrainImages:]
	trainLabels = labels[0:nTrainImages]
	testLabels = labels[nTrainImages:]

	trainSetImageIds = np.zeros((nClasses, nImagesPerClassTrain), dtype='uint32')
	trainSetLabels = np.zeros((nClasses, nImagesPerClassTrain, nClasses))
	#pdb.set_trace()
	for i in range(nClasses):
		consider = trainLabels[:, i] == 1
		curImageIds = trainImages[consider]
		curLabels = trainLabels[consider,:]
		curImageIds, curLabels = shuffleInUnison(curImageIds, curLabels)
		trainSetImageIds[i,:] = np.reshape(curImageIds[0:nImagesPerClassTrain], (nImagesPerClassTrain,))
		trainSetLabels[i, :, :] = curLabels[0:nImagesPerClassTrain]

	testSetImageIds = np.zeros((nClasses, nImagesPerClassTest), dtype='uint32')
	testSetLabels = np.zeros((nClasses, nImagesPerClassTest, nClasses))
	#pdb.set_trace()
	for i in range(nClasses):
		consider = testLabels[:, i] == 1
		curImageIds = testImages[consider]
		curLabels = testLabels[consider,:]
		curImageIds, curLabels = shuffleInUnison(curImageIds, curLabels)
		testSetImageIds[i,:] = np.reshape(curImageIds[0:nImagesPerClassTest], (nImagesPerClassTest,))
		testSetLabels[i, :, :] = curLabels[0:nImagesPerClassTest]
	
	trainSetImageIds = np.reshape(trainSetImageIds, (nClasses*nImagesPerClassTrain,))
	trainSetLabels = np.reshape(trainSetLabels, (nClasses*nImagesPerClassTrain, nClasses))
	testSetImageIds = np.reshape(testSetImageIds, (nClasses*nImagesPerClassTest,))
	testSetLabels = np.reshape(testSetLabels, (nClasses*nImagesPerClassTest,nClasses))
	return trainSetImageIds, trainSetLabels, testSetImageIds, testSetLabels

def removeOutliers(vecs, threshold):
	meanVector = np.zeros((300,))
	curVecs = np.asarray(vecs, dtype='float32')
	isNotZero = np.sum(curVecs == 0, axis = -1) != 300
	curVecs = curVecs[isNotZero]
	S = 1.0 - getCosineSimilarity(curVecs, batchSize=1, save=False, getFullMatrix=True)
	toConsider = np.sum(S < 0.0, axis=-1) != S.shape[0] -1 
	toConsider = curVecs[toConsider]
	if toConsider.shape[0] != 0:
		meanVector = np.mean(toConsider, axis=0)
	else:
		meanVector = np.mean(curVecs, axis=0)
	if np.sum(np.isnan(meanVector))!=0:
		pdb.set_trace()
	return meanVector
