import numpy as np
np.random.seed(42)
import random as rn
rn.seed(12345)

import pdb
from scipy import io
import h5py
# from skimage.transform import resize
from scipy.misc import imresize as resize
# import sys
# sys.path.insert(0, '/media/vrpg/parent/vijetha/CVPR_2018_multiHashing')
import numpy as np
np.random.seed(42)
import random as rn
rn.seed(12345)

def convertLevelDbtoHdf5(sourcePath, targetPath):
	"""
	Description:
		converts a level-db file (generated by caffe) to a hdf5 file.
	Args:
		sourcePath - file path at which the level-db file is present.
		targetPath - file path to which the hdf5 file is to be stored.
	Outputs:
		stores a hdf5file at the give path 'sourcePath'
	"""
	import caffe
	import leveldb
	from caffe.proto import caffe_pb2
	dbData = leveldb.LevelDB(sourcePath)
	datum = caffe_pb2.Datum()

	data = []
	for key, value in dbData.RangeIter():
	    datum.ParseFromString(value)
	    data.append(caffe.io.datum_to_array(datum))
	data = np.array(data)
	# pdb.set_trace()
	data = np.reshape(data, (data.shape[0], data.shape[1]))
	f = h5py.File(targetPath, 'w')
	f.create_dataset('data', data=data)
	f.close()


def mergeHdf5Files(listOfFilesToMerge, nameOfNewFile):
	fNew = h5py.File(nameOfNewFile, 'w')
	for i in range(len(listOfFilesToMerge)):
		fOld = h5py.File(listOfFilesToMerge[i][0], 'r')
		data = fOld['data'][:]
		fOld.close()
		fNew.create_dataset(listOfFilesToMerge[i][1], data=data)
		del data
	fNew.close()

def getRealValuedToCode(x, threshold):
	x = np.array(x>threshold, dtype='int32')
	return x

def oneHotVectors(x, n):
	if np.max(x) == 0:
		raise AssertionError()
	m = max(x.shape)
	y = np.zeros((m, n))
	x = np.reshape(x, (m, ))
	y[np.arange(m), x] = 1
	return y

def computemAP(hammingRank, groundTruthSimilarity, trackPrec = False):
	[Q, N] = hammingRank.shape
	pos = np.arange(N)+1
	MAP = 0
	numSucc = 0
	if trackPrec:
		rareClsVsPrec = []
	for i in range(Q):
		ngb = groundTruthSimilarity[i, np.asarray(hammingRank[i,:], dtype='int32')]
		nRel = np.sum(ngb)
		if nRel > 0:
			prec = np.divide(np.cumsum(ngb), pos)
			prec = prec
			ap = np.mean(prec[np.asarray(ngb, dtype='bool')])
			MAP = MAP + ap
			numSucc = numSucc + 1
			if trackPrec:
				rareClsVsPrec.append((nRel, ap))
	MAP = float(MAP)/numSucc
	if trackPrec:
		import scipy.io as sio 
		sio.savemat('rareClsVsPrec.mat',{'rareClsVsPrec':rareClsVsPrec})
	return MAP


def computeSimilarity(queryLabels, databaseLabels, typeOfData='singleLabelled'):
	groundTruthSimilarityMatrix = np.zeros((queryLabels.shape[0], databaseLabels.shape[0]))
	if typeOfData=='singleLabelled':
		queryLabels = np.reshape(queryLabels, (max(queryLabels.shape),))
		databaseLabels = np.reshape(databaseLabels, (max(databaseLabels.shape),))
		for i in range(queryLabels.shape[0]):
			groundTruthSimilarityMatrix[i,:] = queryLabels[i] == databaseLabels
	elif typeOfData=='multiLabelled':
		for i in range(queryLabels.shape[0]):
			curQue = queryLabels[i][:]
			if sum(curQue) != 0:
				threshold = 1
				sim = np.sum(np.logical_and(curQue, databaseLabels), axis=-1)
				den = np.sum(np.logical_or(curQue, databaseLabels), axis=-1)
				groundTruthSimilarityMatrix[i][np.where(sim >= threshold)[0]] = 1
	groundTruthSimilarityMatrix = np.asarray(groundTruthSimilarityMatrix, dtype='float32')
	return groundTruthSimilarityMatrix


def calcHammingRank(queryHashes, databaseHashes):
	hammingDist = np.zeros((queryHashes.shape[0], databaseHashes.shape[0]))
	hammingRank = np.zeros((queryHashes.shape[0], databaseHashes.shape[0]))
	for i in range(queryHashes.shape[0]):
		hammingDist[i] = np.reshape(np.sum(np.abs(queryHashes[i] - databaseHashes), axis=1), (databaseHashes.shape[0], ))
		hammingRank[i] = np.argsort(hammingDist[i])
	return hammingDist, hammingRank


def prAtK(hammingDist, groundTruthSimilarity, k):
	countOrNot = np.array(hammingDist <= k, dtype='int32')
	newSim = np.multiply(groundTruthSimilarity, countOrNot)
	countOrNot = countOrNot + 0.000001
	#pdb.set_trace()
	prec = np.mean(np.divide(np.sum(newSim, axis=-1), np.sum(countOrNot, axis=-1)))
	rec = np.mean(np.divide(np.sum(newSim, axis=-1), np.sum(groundTruthSimilarity, axis=-1)))
	return (prec, rec)


def writeHashingResultsToCsv(results, fileName, mode, approaches, datasets, nBits, toCompute):
	import csv
	with open(fileName, mode) as csvfile:
		mywriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)
		for z in range(len(toCompute)):
			mywriter.writerow([toCompute[z]])
			row = ['Approaches']
			for ii in range(len(datasets)):
				for jj in range((len(nBits))):
					if jj == 0:
						row.append(datasets[ii])
					else:
						row.append('-')
			mywriter.writerow(row)
			row = [' ']
			for i in range(len(datasets)):
				for j in range((len(nBits))):
					row.append(nBits[j])
			mywriter.writerow(row)
			for y in range(len(approaches)):
				row = []
				row.append(approaches[y])
				for x in range(len(datasets)):
					for w in range(len(nBits)):
						num = round(results[y, x, w, z], 4)
						if num != -100:
							row.append(str(num))
						else:
							row.append('-')
				mywriter.writerow(row)
			mywriter.writerow([' '])

def numUniqueHashes(x):
	y = np.unique(x, axis=0)
	return y.shape[0]

def getShannonEntropy(x):
	raise NotImplementedError

def getAvgHashHistogram(hammingDist, nBits=12):
	finalHist = np.zeros((nBits,))
	for i in range(hammingDist.shape[0]):
		# pdb.set_trace()
		finalHist = finalHist + np.histogram(hammingDist[i,:], nBits)[0]
	return finalHist

def getCosineSimilarity(x, batchSize=50, save=True, getFullMatrix=False):
	from scipy.spatial.distance import cdist
	distances = np.zeros((x.shape[0], x.shape[0]), dtype='float32')
	for i in range(int(x.shape[0]/batchSize)):
		for j in range(int(x.shape[0]/batchSize)):
			if np.sum(distances[j*batchSize:(j+1)*batchSize, i*batchSize:(i+1)*batchSize]) == 0:
				dst = cdist(x[i*batchSize:(i+1)*batchSize] , x[j*batchSize:(j+1)*batchSize] ,  'cosine')
				distances[i*batchSize:(i+1)*batchSize, j*batchSize:(j+1)*batchSize] = dst
			elif getFullMatrix:
				distances[i*batchSize:(i+1)*batchSize, j*batchSize:(j+1)*batchSize] = distances[j*batchSize:(j+1)*batchSize, i*batchSize:(i+1)*batchSize]
	return distances

def getWeightShapesFromModel(model, library='Keras'):
	weightShapes=[]
	print("Printing From Model")
	if library == 'Keras':
		nLayers = len(model.layers)
		for i in range(nLayers):
			nParamSets = len(model.layers[i].get_weights())
			assert nParamSets%2 == 0
			for j in range(int(nParamSets/2)):
				weightShapes.append([model.layers[i].get_weights()[2*j].shape, model.layers[i].get_weights()[2*j+1].shape])
				print(weightShapes[-1])
	return weightShapes


def getWeightShapesFromH5(fileName):
	f = h5py.File(fileName, 'r')
	allKeys=[k for k in f.keys()]
	weightShapes=[]
	print("Printing From saved weights")
	for layerNumber in range(len(allKeys)):
		subKeys = [k for k in f[allKeys[layerNumber]].keys()]
		assert len(subKeys)%2 == 0
		for i in range(int(len(subKeys)/2)):
			weightShapes.append([f[allKeys[layerNumber]][subKeys[2*i]][:].shape, f[allKeys[layerNumber]][subKeys[2*i+1]][:].shape])
			print(weightShapes[-1])
	f.close()
	return weightShapes

def getWeightShapesFromNpyFile(fileName):
	weights = np.load(fileName, encoding = 'bytes').item()
	weightKeys = [key for key in weights.keys() if '__' not in key]
	for i in range(len(weightKeys)):
		curDs = weights[weightKeys[i]]
		for j in range(len(curDs)):
			print(curDs[j].shape)

def convertThtoTf(srcFileName, dstFileName):
	from keras.utils.conv_utils import convert_kernel
	from shutil import copyfile
	copyfile(srcFileName, dstFileName)
	f = h5py.File(dstFileName, 'r+')
	allKeys=[k for k in f.keys()]
	for layerNumber in range(len(allKeys)):
		if 'conv' in allKeys[layerNumber]:
			subKeys = [k for k in f[allKeys[layerNumber]].keys()]
			for i in range(len(subKeys)):
				original_w = f[allKeys[layerNumber]][subKeys[i]][:]
				if i == 0:
					abcd = f[allKeys[layerNumber]][subKeys[0]]
					del f[allKeys[layerNumber]][subKeys[0]]
					pdb.set_trace()
					original_w = np.transpose(convert_kernel(original_w), (2, 3, 1, 0))
					data = f[allKeys[layerNumber]].create_dataset(subKeys[i], original_w.shape)
					data = original_w
	f.close()


def matToHdf5(srcFileName, dstFileName):
	data = io.loadmat(srcFileName)
	datasets = [key for key in data.keys() if '__' not in key]
	f = h5py.File(dstFileName, 'w')
	for i in range(len(datasets)):
		if datasets[i] == 'B_trn':
			f.create_dataset('train_hashes', data=data[datasets[i]])
		elif datasets[i] == 'B_tst':
			f.create_dataset('test_hashes', data=data[datasets[i]])
		elif datasets[i] == 'WtrueTestTraining':
			f.create_dataset('groundTruthSimilarity', data=data[datasets[i]])
		else:
			f.create_dataset(datasets[i], data=data[datasets[i]])
	f.close()

def prepImageData(images, chOrder='channelsLast', resizeHeight=256, resizeWidth=256, meanSubtractOrder='BGR', bsAxis = 0):
	images = batchSizeFirst(images, bsAxis)
	if chOrder == 'channelsLast':
		images = channelsFirstToLast(images)
	elif chOrder == 'channelsFirst':
		images = channelsLastToFirst(images)
	images = resizeImages(images, resizeHeight, resizeWidth)
	images = meanSubtract(images, meanSubtractOrder)
	return images

def im2Arr(path, chOrder='channelsLast', resizeHeight=256, resizeWidth=256, meanSubtractOrder='BGR'):
	import cv2
	try:
		image = cv2.imread(path)
		image = np.expand_dims(image, axis=0)
		image = prepImageData(image, chOrder='channelsLast', resizeHeight=256, resizeWidth=256, meanSubtractOrder='BGR')
		return image
	except:
		return None

def prepLabelData(labels, sourceType='uint', targetType='uint'):
	batchSize = max(labels.shape)
	if sourceType== 'uint' and targetType == 'uint':
		labels = np.reshape(labels, (batchSize, ))
	elif sourceType == 'oneHot' and targetType == 'uint':
		labels = np.array(np.argmax(labels, axis=-1), dtype='int32')
	else:
		raise NotImplementedError
	return labels

def makeCIFAR10(srcFileName, dstFileName, printInfo = True, 
				batchSize = 1000, 
				chOrder='channelsLast', 
				resizeHeight=256, 
				resizeWidth=256, 
				meanSubtractOrder='BGR', 
				labelSourceType='uint', 
				labelTargetType='uint'):
	fSrc = h5py.File(srcFileName, 'r')
	fDst = h5py.File(dstFileName, 'w')
	datasets = [key for key in fSrc.keys() if '__' not in key]
	for i in range(len(datasets)):
		dShape =  fSrc[datasets[i]][:].shape
		bs = np.max(dShape)
		bsOrder = np.argmax(dShape)
		print("Processing Dataset -"+str(datasets[i]))
		print("Total Samples - "+str(bs))
		if len(dShape) == 4 and np.max(fSrc[datasets[i]][:]) > 200:
			if chOrder == 'channelsFirst':
				fDst.create_dataset(datasets[i], (bs, 3, resizeHeight, resizeWidth))
			elif chOrder == 'channelsLast':
				fDst.create_dataset(datasets[i], (bs, resizeHeight, resizeWidth, 3))
			nBatches = int(bs/batchSize)
			for j in range(nBatches):
				if printInfo:
					print("# processed images - "+str(j*batchSize))
				if bsOrder == 3:
					image = fSrc[datasets[i]][:,:,:,j*batchSize:(j+1)*batchSize]
				elif bsOrder == 0:
					image = fSrc[datasets[i]][j*batchSize:(j+1)*batchSize,:,:,:]
				if len(image.shape) != 4:
					image = np.expand_dims(image, axis=bsOrder)
				data = prepImageData(image, chOrder, resizeHeight, resizeWidth, meanSubtractOrder, bsOrder)
				fDst[datasets[i]][j*batchSize:(j+1)*batchSize, ...] = data  # Assuming batch size always at the first dimension
		else:
			data = prepLabelData(fSrc[datasets[i]][:], labelSourceType, labelTargetType)
			fDst.create_dataset(datasets[i], data=data)
	fSrc.close()
	fDst.close()

def cleanH5(srcFile, dstFile, chOrder = 'channelsLast', batchSize=1000):
	pdb.set_trace()
	fSrc = h5py.File(srcFile, 'r')
	fDst = h5py.File(dstFile, 'w')
	datasets = [key for key in fSrc.keys() if '__' not in key]
	for i in range(len(datasets)):
		print("At dataset "+str(datasets[i]))
		ind = datasets[i].find('_')
		if 'img' in datasets[i]:
			newName = datasets[i][0:ind+1]+'data'
			bs = fSrc[datasets[i]][:].shape[0]
			fDst.create_dataset(newName, (bs, 256, 256, 3))
			for j in range(int(bs/batchSize)+1):
				if j%5 == 0:
					print("At "+str(j))
				data = fSrc[datasets[i]][j*batchSize:(j+1)*batchSize]
				data = np.transpose(data, (0, 2, 3, 1))
				fDst[newName][j*batchSize:(j+1)*batchSize, ...] = data
		elif 'label' in datasets[i]:
			newName = datasets[i][0:ind+1]+'labels'
			data = fSrc[datasets[i]][:]
			fDst.create_dataset(newName, data=data)
			del data
		elif 'vector' in datasets[i]:
			newName = datasets[i][0:ind+1]+'vectors'
			data = fSrc[datasets[i]][:]
			fDst.create_dataset(newName, data=data)
			del data
	fDst.close()
	fSrc.close()


def makeNUS(srcFileName, dstFileName, printInfo = True, 
				batchSize = 1000, 
				chOrder='channelsLast', 
				resizeHeight=256, 
				resizeWidth=256,
				meanSubtractOrder='BGR', 
				labelSourceType='uint', 
				labelTargetType='uint'):
	fSrc = h5py.File(srcFileName, 'r')
	fDst = h5py.File(dstFileName, 'w')
	datasets = [key for key in fSrc.keys() if '__' not in key]
	for i in range(len(datasets)):
		dShape =  fSrc[datasets[i]][:].shape
		bs = np.max(dShape)
		bsOrder = np.argmax(dShape)
		print("Processing Dataset -"+str(datasets[i]))
		print("Total Samples - "+str(bs))
		if len(dShape) == 4 and np.max(fSrc[datasets[i]][:]) > 200:
			if chOrder == 'channelsFirst':
				fDst.create_dataset(datasets[i], (bs, 3, resizeHeight, resizeWidth))
			elif chOrder == 'channelsLast':
				fDst.create_dataset(datasets[i], (bs, resizeHeight, resizeWidth, 3))
			nBatches = int(bs/batchSize)
			for j in range(nBatches):
				if printInfo:
					print("# processed images - "+str(j*batchSize))
				if bsOrder == 3:
					image = fSrc[datasets[i]][:,:,:,j*batchSize:(j+1)*batchSize]
				elif bsOrder == 0:
					image = fSrc[datasets[i]][j*batchSize:(j+1)*batchSize,:,:,:]
				if len(image.shape) != 4:
					image = np.expand_dims(image, axis=bsOrder)
				data = prepImageData(image, chOrder, resizeHeight, resizeWidth, meanSubtractOrder)
				fDst[datasets[i]][j*batchSize:(j+1)*batchSize, ...] = data  # Assuming batch size always at the first dimension
		else:
			data = prepLabelData(fSrc[datasets[i]][:], labelSourceType, labelTargetType)
			fDst.create_dataset(datasets[i], data=data)
	fSrc.close()
	fDst.close()
	
def getTagMatrix(excelFileName):
	raise NotImplementedError

def resizeImages(images, resizeHeight=256, resizeWidth = 256):
	order = images.shape
	ch = order.index(3)
	if ch == 1:
		images = channelsFirstToLast(images)
	resizedImages = np.zeros((images.shape[0], resizeHeight, resizeWidth, 3))
	for i in range(resizedImages.shape[0]):
		resizedImages[i,:,:,:] = resize(images[i], (resizeHeight, resizeWidth))
	if np.max(resizedImages) <= 1:
		resizedImages = 255.0*resizedImages
	if ch == 1:
		resizedImages = channelsLastToFirst(resizedImages)
	return resizedImages

def batchSizeFirst(images, bs=0):
	order = images.shape
	assert len(order) == 4
	if bs == 3:
		images = np.transpose(images, (3, 0, 1, 2))
	elif bs == 0:
		pass
	else:
		raise NotImplementedError
	return images

def cropImages(images, cropHeight=227, cropWidth=227):
	order = images.shape
	ch = order.index(3)
	if ch == 1:
		images = channelsFirstToLast(images)
	croppedImages = np.zeros((images.shape[0], cropHeight, cropWidth, 3))
	for i in range(croppedImages.shape[0]):
		randX = np.random.randint(images.shape[1]-cropHeight)
		randY = np.random.randint(images.shape[2]-cropWidth)
		croppedImages[i,:,:,:] = images[i,randX:randX+cropHeight,randY:randY+cropWidth,:]
	if ch == 1:
		croppedImages = channelsLastToFirst(croppedImages)
	return croppedImages

def channelsFirstToLast(images):
	order = images.shape
	ch = order.index(3)
	if ch == 1:
		images = np.transpose(images, (0, 2, 3, 1))
	return images

def channelsLastToFirst(images):
	order = images.shape
	ch = order.index(3)
	if ch == 3:
		images = np.transpose(images, (0, 3, 1, 2))
	return images

def meanSubtract(images, sourceDataSet='IMAGENET', order='RGB'):
	chOrder = images.shape
	ch = chOrder.index(3)	
	if ch == 1:
		images = channelsFirstToLast(images)
	if order == 'RGB':
		#in RGB order
	    images[:, :, :, 0] -= 123.68
	    images[:, :, :, 1] -= 116.779
	    images[:, :, :, 2] -= 103.939 # values copied from https://github.com/heuritech/convnets-keras/blob/master/convnetskeras/convnets.py
	elif order == 'BGR':
	    images[:, :, :, 0] -= 103.939
	    images[:, :, :, 1] -= 116.779
	    images[:, :, :, 2] -= 123.68 # values copied from https://github.com/heuritech/convnets-keras/blob/master/convnetskeras/convnets.py
	if ch == 1:
		images = channelsLastToFirst(images)
	return images


def shuffleInUnison(images, labels):
	perm = np.random.permutation(images.shape[0])
	images = images[perm]
	labels = labels[perm]
	return images, labels

def emailSender(mystr, sendEmail=False):
	if sendEmail:
		import smtplib
		fromaddr = '****'
		toaddrs  = '****'
		SUBJECT = "From Python Program"
		message = """\
		From: %s
		To: %s
		Subject: %s

		%s
		""" % (fromaddr, ", ".join(toaddrs), SUBJECT, mystr)
		username = '****'
		password = '****'
		server = smtplib.SMTP('smtp.gmail.com:587')
		server.starttls()
		server.login(username,password)
		server.sendmail(fromaddr, toaddrs, message)
		server.quit()

def checkIfWeightsAreNotLost(model_1, model_2, layerList):
	for i in range(len(layerList)):
		sameWeights = False
		weights1 = model_1.layers[layerList[i]].get_weights()[0]
		weights2 = model_2.layers[layerList[i]].get_weights()[0]
		if weights1.shape != weights2.shape:
			print("Weights Shapes did not match")
			break
		else:
			totalNumberOfWeights = getTotalWeights(weights1.shape)
			if np.sum(weights1 == weights2) != totalNumberOfWeights:
				print("Weights are different")
				break
			else:
				sameWeights = True
	return sameWeights

def getTotalWeights(weightsShape):
	totalWeights = 1
	for i in range(len(weightsShape)):
		totalWeights = totalWeights*weightsShape[i]
	return totalWeights

def computeAccuracy(predictions, groundTruths):
	predictions = prepLabelData(predictions, sourceType='oneHot', targetType='uint')
	groundTruths = prepLabelData(groundTruths, sourceType='uint', targetType='uint')
	acc = np.sum((predictions == groundTruths))*100/predictions.shape[0]
	return acc

def loadJsonFile(fileName):
	import json
	f = open(fileName, 'r')
	allData = json.load(f)
	f.close()
	return allData

def getTagVectorsForEachImage(data, imId=None, ind=None):
	if imId != None:
		ind = [i for i,x in enumerate(data) if x[0] == imId]
	elif ind != None:
		ind = ind
	else:
		ind = np.random.randint(len(data))
	vecs = data[ind][2]
	vecs = np.asarray(vecs)
	return vecs

def getSvd(S):
	import numpy.linalg as la
	try:
		u, e, v = la.svd(S, full_matrices=True)
	except:
		u=0
		e=0
		v=0
	return u, e, v

def makeTrainTestSplits(imageIds, labels, labelType = 'oneHot', nImagesPerClassTrain=500, nImagesPerClassTest = 100):
	if labelType == 'oneHot':
		nClasses = labels.shape[1]
	nTrainImages = int(imageIds.shape[0]*0.7)
	trainImages = imageIds[0:nTrainImages]
	testImages = imageIds[nTrainImages:]
	trainLabels = labels[0:nTrainImages]
	testLabels = labels[nTrainImages:]

	trainSetImageIds = np.zeros((nClasses, nImagesPerClassTrain), dtype='uint32')
	trainSetLabels = np.zeros((nClasses, nImagesPerClassTrain, nClasses))
	for i in range(nClasses):
		consider = trainLabels[:, i] == 1
		curImageIds = trainImages[consider]
		curLabels = trainLabels[consider,:]
		curImageIds, curLabels = shuffleInUnison(curImageIds, curLabels)
		trainSetImageIds[i,:] = np.reshape(curImageIds[0:nImagesPerClassTrain], (nImagesPerClassTrain,))
		trainSetLabels[i, :, :] = curLabels[0:nImagesPerClassTrain]

	testSetImageIds = np.zeros((nClasses, nImagesPerClassTest), dtype='uint32')
	testSetLabels = np.zeros((nClasses, nImagesPerClassTest, nClasses))
	for i in range(nClasses):
		consider = testLabels[:, i] == 1
		curImageIds = testImages[consider]
		curLabels = testLabels[consider,:]
		curImageIds, curLabels = shuffleInUnison(curImageIds, curLabels)
		testSetImageIds[i,:] = np.reshape(curImageIds[0:nImagesPerClassTest], (nImagesPerClassTest,))
		testSetLabels[i, :, :] = curLabels[0:nImagesPerClassTest]
	
	trainSetImageIds = np.reshape(trainSetImageIds, (nClasses*nImagesPerClassTrain,))
	trainSetLabels = np.reshape(trainSetLabels, (nClasses*nImagesPerClassTrain, nClasses))
	testSetImageIds = np.reshape(testSetImageIds, (nClasses*nImagesPerClassTest,))
	testSetLabels = np.reshape(testSetLabels, (nClasses*nImagesPerClassTest,nClasses))
	return trainSetImageIds, trainSetLabels, testSetImageIds, testSetLabels

def removeOutliers(vecs, threshold):
	meanVector = np.zeros((300,))
	curVecs = np.asarray(vecs, dtype='float32')
	isNotZero = np.sum(curVecs == 0, axis = -1) != 300
	curVecs = curVecs[isNotZero]
	S = 1.0 - getCosineSimilarity(curVecs, batchSize=1, save=False, getFullMatrix=True)
	toConsider = np.sum(S < 0.0, axis=-1) != S.shape[0] -1 
	toConsider = curVecs[toConsider]
	if toConsider.shape[0] != 0:
		meanVector = np.mean(toConsider, axis=0)
	else:
		meanVector = np.mean(curVecs, axis=0)
	if np.sum(np.isnan(meanVector))!=0:
		pdb.set_trace()
	return meanVector

def readImageBatch(imagesPaths, folderName, resizeHeight=256, resizeWidth=256, cropHeight=227, cropWidth=227, sourceDataSet='IMAGENET', orderOfChannels='BGR'):
	'''
	Inputs:
	Outputs:
	'''
	imagesBatch = np.zeros((len(imagesPaths), 3, cropHeight, cropWidth))
	extracedImages = []
	counter = 0
	for i in range(len(imagesPaths)):
		try:
			image = readImage(folderName + imagesPaths[i])
			#pdb.set_trace()
			if np.sum(image != None):
				imagesBatch[counter,:,:,:] = image[0,:,:,:]
				counter = counter + 1
				extracedImages.append((i, imagesPaths[i]))
		except:
			pass
	imagesBatch = imagesBatch[0:counter,:,:,:]
	return imagesBatch, extracedImages
		
def getTriplets(nSamples, labels, batch_size):
	triplets = np.zeros((batch_size, 3), dtype='int32')
	ind = 0
	toFill = np.sum(np.sum(triplets, axis=-1)==0)
	while (toFill != 0):
		contextSamples = np.random.permutation(nSamples)
		contextSamples = contextSamples[0:batch_size]
		posSamples = np.random.permutation(nSamples)
		posSamples = posSamples[0:batch_size]
		negSamples = np.random.permutation(nSamples)
		negSamples = negSamples[0:batch_size]	
		L = labels[contextSamples]
		L_plus = labels[posSamples]
		L_minus = labels[negSamples]
		d_plus = np.reshape(np.sum(np.abs(L - L_plus), axis=1), (batch_size))
		d_minus = np.reshape(np.sum(np.abs(L - L_minus), axis=1), (batch_size))
		atleastOneCommonLabel_plus = np.sum(np.logical_and(L, L_plus), axis=1)
		atleastOneCommonLabel_minus = np.sum(np.logical_and(L, L_minus), axis=1)
		correct = np.logical_and(d_plus < d_minus, atleastOneCommonLabel_plus)
		reverse = np.logical_and(d_minus < d_plus, atleastOneCommonLabel_minus)
		nCorrect = np.sum(correct)
		nReverse = np.sum(reverse)
		triplets[ind:ind+nCorrect,0] = contextSamples[correct][0:toFill]
		triplets[ind:ind+nCorrect,1] = posSamples[correct][0:toFill]
		triplets[ind:ind+nCorrect,2] = negSamples[correct][0:toFill]
		ind = ind + nCorrect
		toFill = np.sum(np.sum(triplets, axis=-1)==0)
		triplets[ind:ind+nReverse,0] = contextSamples[reverse][0:toFill]
		triplets[ind:ind+nReverse,1] = negSamples[reverse][0:toFill]
		triplets[ind:ind+nReverse,2] = posSamples[reverse][0:toFill]
		ind = ind + nReverse
		toFill = np.sum(np.sum(triplets, axis=-1)==0)
	return triplets

def readImage(imageName, 
			libraryName='cv2', resizeHeight=256, resizeWidth=256, cropHeight=227, cropWidth=227, sourceDataSet='IMAGENET', orderOfChannels='BGR'):
	'''
    reads the image, resizes, crops and aligns channels with respect to the order inputted

    Inputs: 
        The path to the image

    Returns: 
        A 4 D numpy array of shape [Batch Size(which is 1 here), Channels, Image Height, Image Width]
    '''
	try:
	    image = cv2.imread(imageName)
	    image = np.transpose(image, (2, 0, 1))
	    image = np.expand_dims(image, axis=0)
	    image = resizeImages(image, resizeHeight, resizeWidth)
	    image = meanSubtract(image, sourceDataSet, orderOfChannels)
	    image = cropImages(image, cropHeight, cropWidth)
	except:
		print("skipped reading image")
		image = None
	return image
	

def getData(dataset='CIFAR10', channels_last=True):
	if dataset == 'CIFAR10':
		#This matrix is made by the MATLAB/MatConvNet/DPSH_IJCAI_ version 1.0_beta23 code. As per the code, the data should be in RGB format(verified visually) 
		data = sio.loadmat('./datasets/cifar-10.mat')

	trainData = data['train_data']
	trainLabels = data['train_L']
	queryData = data['test_data']
	queryLabels = data['test_L']
	galleryData = data['data_set']
	galleryLabels = data['dataset_L']
	if channels_last:
		trainData = np.transpose(trainData, (3, 0, 1, 2))
		queryData = np.transpose(queryData, (3, 0, 1, 2))
		galleryData = np.transpose(galleryData, (3, 0, 1, 2))
	else:
		raise NotImplementedError
	return trainData, trainLabels, queryData, queryLabels, galleryData, galleryLabels

def makeDataSet(imageIds, labels, labelType = 'oneHot', nImagesPerClassTrain=500, nImagesPerClassTest = 100):
	if labelType == 'oneHot':
		nClasses = labels.shape[1]
	nTrainImages = int(imageIds.shape[0]*0.7)
	trainImages = imageIds[0:nTrainImages]
	testImages = imageIds[nTrainImages:]
	trainLabels = labels[0:nTrainImages]
	testLabels = labels[nTrainImages:]

	trainSetImageIds = np.zeros((nClasses, nImagesPerClassTrain), dtype='uint32')
	trainSetLabels = np.zeros((nClasses, nImagesPerClassTrain, nClasses))
	for i in range(nClasses):
		consider = trainLabels[:, i] == 1
		curImageIds = trainImages[consider]
		curLabels = trainLabels[consider,:]
		curImageIds, curLabels = shuffleInUnison(curImageIds, curLabels)
		trainSetImageIds[i,:] = np.reshape(curImageIds[0:nImagesPerClassTrain], (nImagesPerClassTrain,))
		trainSetLabels[i, :, :] = curLabels[0:nImagesPerClassTrain]

	testSetImageIds = np.zeros((nClasses, nImagesPerClassTest), dtype='uint32')
	testSetLabels = np.zeros((nClasses, nImagesPerClassTest, nClasses))
	for i in range(nClasses):
		consider = testLabels[:, i] == 1
		curImageIds = testImages[consider]
		curLabels = testLabels[consider,:]
		curImageIds, curLabels = shuffleInUnison(curImageIds, curLabels)
		testSetImageIds[i,:] = np.reshape(curImageIds[0:nImagesPerClassTest], (nImagesPerClassTest,))
		testSetLabels[i, :, :] = curLabels[0:nImagesPerClassTest]
	
	trainSetImageIds = np.reshape(trainSetImageIds, (nClasses*nImagesPerClassTrain,))
	trainSetLabels = np.reshape(trainSetLabels, (nClasses*nImagesPerClassTrain, nClasses))
	testSetImageIds = np.reshape(testSetImageIds, (nClasses*nImagesPerClassTest,))
	testSetLabels = np.reshape(testSetLabels, (nClasses*nImagesPerClassTest,nClasses))
	return trainSetImageIds, trainSetLabels, testSetImageIds, testSetLabels

def generatePairs(images, labels, batch_size):
	n_classes = np.unique(labels).shape[0]
	images_classwise = np.zeros((n_classes, images.shape[0]/n_classes, images.shape[1], images.shape[2], images.shape[3]))
	for i in range(n_classes):
		curClass = labels == i
		images_classwise[i,:,:,:,:] = images[curClass,:,:,:]
	randomLabels = np.random.randint(10, size=batch_size)
	simLabels = randomLabels[0:batch_size/2]
	dissimLabels = randomLabels[batch_size/2:batch_size]
	imagePairs = []
	similarity = []
	queryLabs = []
	databaseLabs = []
	for i in range(len(simLabels)):
		randomImgNums = np.random.randint(images.shape[0]/n_classes, size=2)
		imagePairs.append([images_classwise[simLabels[i], randomImgNums[0], :, :, :], images_classwise[simLabels[i], randomImgNums[1], :, :, :]])
		similarity.append(1)
		queryLabs.append(simLabels[i])
		databaseLabs.append(simLabels[i])
	for i in range(len(dissimLabels)):
		randomImgNums = np.random.randint(images.shape[0]/n_classes, size=2)
		secondImageClass = dissimLabels[i]
		while(secondImageClass==dissimLabels[i]):
			secondImageClass = np.random.randint(n_classes)
		imagePairs.append([images_classwise[dissimLabels[i], randomImgNums[0], :, :, :], images_classwise[secondImageClass, randomImgNums[1], :, :, :]])
		similarity.append(0)
		queryLabs.append(dissimLabels[i])
		databaseLabs.append(secondImageClass)
	imagePairs = np.array(imagePairs)
	similarity = np.array(similarity)
	return imagePairs, similarity, np.asarray(queryLabs), np.asarray(databaseLabs)

def prepareData(dataset='CIFAR10'):
	trainData, trainLabels, queryData, queryLabels, galleryData, galleryLabels = getData(dataset=dataset)
	return trainData, trainLabels, queryData, queryLabels, galleryData, galleryLabels

def multiLabelGetVectors(data, dim=300, nClasses=81, nTags=1000, method='mean'):
	vecMat = np.zeros((len(data), dim))
	labels = np.zeros((len(data), nClasses))
	images = np.zeros((len(data), 1))
	tags = np.zeros((len(data), nTags))
	for i in range(len(data)):
		curRec = data[i]
		curVector = np.zeros((300, ))
		images[i] = curRec[0]
		labels[i] = curRec[1]
		if method == 'mean':
			for j in range(len(curRec[2])):
				curVector = curVector + curRec[2][j]
				tags[i][int(curRec[2][j][1])] = 1
			vecMat[i][:] = curVector/float(len(curRec[2]))
		elif method == 'idf':
			avg = 0
			for j in range(len(curRec[2])):
				curVector = curVector +[x* curRec[2][j][2] for x in curRec[2][j][0]]
				tags[i][int(curRec[2][j][1])] = 1
				avg = avg + curRec[2][j][2]
			vecMat[i][:] = curVector/float(avg)
		elif method == 'minFreq':
			minFreq = 100
			minFrqIndex = -100
			for j in range(len(curRec[2])):
				if curRec[2][j][2] < minFreq:
					tags[i][int(curRec[2][j][1])] = 1
					minFreq = curRec[2][j][2]
					minFreqIndex = j
			vecMat[i][:] = curRec[2][minFreqIndex][0]
		elif method == 'cutFreq':
			avg = 0.00001
			for j in range(len(curRec[2])):
				if curRec[2][j][2] > 5.3 and curRec[2][j][2] < 8.2:
					curVector = curVector +[x* curRec[2][j][2] for x in curRec[2][j][0]]
					tags[i][int(curRec[2][j][1])] = 1
					avg = avg + curRec[2][j][2]
			vecMat[i][:] = curVector/float(avg)
	images = np.array(images, dtype='uint32')			
	return (images, labels, vecMat, tags)

def multiLabelGetVectorsNUS(data, dim=300, nClasses=81, nTags=1000, method='mean'):
	vecMat = np.zeros((len(data), dim))
	labels = np.zeros((len(data), nClasses))
	images = np.zeros((len(data), 1))
	tags = []
	for i in range(len(data)):
		curRec = data[i]

		curVector = np.zeros((300, ))
		images[i] = curRec[0]
		labels[i] = curRec[1]
		if method == 'mean':
			for j in range(len(curRec[2])):
				curVector = curVector + curRec[2][j]
			vecMat[i][:] = curVector/float(len(curRec[2]))
			tags.append(curRec[3])
		elif method == 'idf':
			avg = 0
			for j in range(len(curRec[2])):
				curVector = curVector +[x* curRec[2][j][2] for x in curRec[2][j][0]]
				avg = avg + curRec[2][j][2]
			vecMat[i][:] = curVector/float(avg)
		elif method == 'minFreq':
			minFreq = 100
			minFrqIndex = -100
			for j in range(len(curRec[2])):
				if curRec[2][j][2] < minFreq:
					minFreq = curRec[2][j][2]
					minFreqIndex = j
			vecMat[i][:] = curRec[2][minFreqIndex][0]
		elif method == 'cutFreq':
			avg = 0.00001
			for j in range(len(curRec[2])):
				if curRec[2][j][2] > 5.3 and curRec[2][j][2] < 8.2:
					curVector = curVector +[x* curRec[2][j][2] for x in curRec[2][j][0]]
					avg = avg + curRec[2][j][2]
			vecMat[i][:] = curVector/float(avg)
	images = np.array(images, dtype='uint32')			
	return (images, labels, vecMat, tags)

def multiLabelGetVectorsDelete(data, dim=300, nClasses=81, nTags=1000, method='mean'):
	vecMat = np.zeros((len(data), dim))
	labels = np.zeros((len(data), nClasses))
	images = np.zeros((len(data), 1))
	tags = np.zeros((len(data), nTags))
	for i in range(len(data)):
		curRec = data[i]
		images[i] = curRec[0]
		labels[i] = curRec[1]
	images = np.array(images, dtype='uint32')			
	return (images, labels)