import numpy as np
import pdb
from scipy import io
import h5py
from skimage.transform import resize
# import sys
# sys.path.insert(0, '/media/vrpg/parent/vijetha/CVPR_2018_multiHashing')


def convertLevelDbtoHdf5(sourcePath, targetPath):
	"""
	Description:
		converts a level-db file (generated by caffe) to a hdf5 file.
	Args:
		sourcePath - file path at which the level-db file is present.
		targetPath - file path to which the hdf5 file is to be stored.
	Outputs:
		stores a hdf5file at the give path 'sourcePath'
	"""
	import caffe
	import leveldb
	from caffe.proto import caffe_pb2
	dbData = leveldb.LevelDB(sourcePath)
	datum = caffe_pb2.Datum()

	data = []
	for key, value in dbData.RangeIter():
	    datum.ParseFromString(value)
	    data.append(caffe.io.datum_to_array(datum))
	data = np.array(data)
	# pdb.set_trace()
	data = np.reshape(data, (data.shape[0], data.shape[1]))
	f = h5py.File(targetPath, 'w')
	f.create_dataset('data', data=data)
	f.close()


def mergeHdf5Files(listOfFilesToMerge, nameOfNewFile):
	"""
	Desc: 

	Args:

	Outputs:

	"""
	fNew = h5py.File(nameOfNewFile, 'w')
	for i in range(len(listOfFilesToMerge)):
		fOld = h5py.File(listOfFilesToMerge[i][0], 'r')
		data = fOld['data'][:]
		fOld.close()
		fNew.create_dataset(listOfFilesToMerge[i][1], data=data)
		del data
	fNew.close()

def getRealValuedToCode(x, threshold):
	"""
	Desc:

	Args:

	Outputs:

	"""
	x = np.array(x>threshold, dtype='int32')
	return x

def oneHotVectors(x):
	"""
	Desc:

	Args:

	Outputs:

	"""
	if np.max(x) == 0:
		raise AssertionError()
	m = max(x.shape)
	n = np.max(x)
	y = np.zeros((m, n))
	x = np.reshape((m, ))
	y[np.arange(m), x] = 1
	return y

def computemAP(hammingRank, groundTruthSimilarity):
	"""
	Desc:

	Args:

	Outputs:

	"""
	[Q, N] = hammingRank.shape
	pos = np.arange(N)+1
	MAP = 0
	numSucc = 0
	for i in range(Q):
		ngb = groundTruthSimilarity[i, np.asarray(hammingRank[i,:], dtype='int32')]
		nRel = np.sum(ngb)
		if nRel > 0:
			prec = np.divide(np.cumsum(ngb), pos)
			#prec = prec[0:5000]
			#pdb.set_trace()
			#ap = np.mean(prec[np.asarray(ngb[0:5000], dtype='bool')])
			prec = prec
			ap = np.mean(prec[np.asarray(ngb, dtype='bool')])
			MAP = MAP + ap
			numSucc = numSucc + 1
	MAP = float(MAP)/numSucc
	return MAP


def computeSimilarity(queryLabels, databaseLabels, typeOfData='singleLabelled'):
	"""
	Desc:

	Args:

	Outputs:

	"""
	groundTruthSimilarityMatrix = np.zeros((queryLabels.shape[0], databaseLabels.shape[0]))
	if typeOfData=='singleLabelled':
		queryLabels = np.reshape(queryLabels, (max(queryLabels.shape),))
		databaseLabels = np.reshape(databaseLabels, (max(databaseLabels.shape),))
		for i in range(queryLabels.shape[0]):
			groundTruthSimilarityMatrix[i,:] = queryLabels[i] == databaseLabels
	elif typeOfData=='multiLabelled':
		for i in range(queryLabels.shape[0]):
			curQue = queryLabels[i][:]
			if sum(curQue) != 0:
				threshold = 1
				sim = np.sum(np.logical_and(curQue, databaseLabels), axis=-1)
				den = np.sum(np.logical_or(curQue, databaseLabels), axis=-1)
				groundTruthSimilarityMatrix[i][np.where(sim >= threshold)[0]] = 1
	groundTruthSimilarityMatrix = np.asarray(groundTruthSimilarityMatrix, dtype='float32')
	return groundTruthSimilarityMatrix


def calcHammingRank(queryHashes, databaseHashes):
	"""
	Desc:

	Args:

	Outputs:

	"""
	hammingDist = np.zeros((queryHashes.shape[0], databaseHashes.shape[0]))
	hammingRank = np.zeros((queryHashes.shape[0], databaseHashes.shape[0]))
	for i in range(queryHashes.shape[0]):
		hammingDist[i] = np.reshape(np.sum(np.abs(queryHashes[i] - databaseHashes), axis=1), (databaseHashes.shape[0], ))
		hammingRank[i] = np.argsort(hammingDist[i])
	return hammingDist, hammingRank


def prAtK(hammingDist, groundTruthSimilarity, k):
	"""
	Desc:

	Args:

	Outputs:

	"""
	countOrNot = np.array(hammingDist <= k, dtype='int32')
	newSim = np.multiply(groundTruthSimilarity, countOrNot)
	countOrNot = countOrNot + 0.000001
	#pdb.set_trace()
	prec = np.mean(np.divide(np.sum(newSim, axis=-1), np.sum(countOrNot, axis=-1)))
	rec = np.mean(np.divide(np.sum(newSim, axis=-1), np.sum(groundTruthSimilarity, axis=-1)))
	return (prec, rec)


def writeCSVHeader(fileName, datasets, nBits, format='Hashing', mode='w'):
	"""
	Desc:

	Args:

	Outputs:

	"""
	import csv
	if format == "Hashing":
		with open(fileName, mode) as csvfile:
			mywriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)
			row = ['Approaches']
			for ii in range(len(datasets)):
				for jj in range((len(nBits))):
					if jj == 0:
						row.append(datasets[ii])
					else:
						row.append(' ')
			mywriter.writerow(row)
			row = [' ']
			for ii in range(len(datasets)):
				for jj in range((len(nBits))):
					row.append(nBits[jj])
			mywriter.writerow(row)


def writeHashingResultsToCsv(results, fileName, mode, approaches, datasets, nBits, toCompute):
	"""
	Desc:

	Args:

	Outputs:

	"""
	import csv
	with open(fileName, mode) as csvfile:
		mywriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)
		for z in range(len(toCompute)):
			mywriter.writerow([toCompute[z]])
			row = ['Approaches']
			for ii in range(len(datasets)):
				for jj in range((len(nBits))):
					if jj == 0:
						row.append(datasets[ii])
					else:
						row.append('-')
			mywriter.writerow(row)
			row = [' ']
			for i in range(len(datasets)):
				for j in range((len(nBits))):
					row.append(nBits[j])
			mywriter.writerow(row)
			for y in range(len(approaches)):
				row = []
				row.append(approaches[y])
				for x in range(len(datasets)):
					for w in range(len(nBits)):
						num = round(results[y, x, w, z], 4)
						if num != -100:
							row.append(str(num))
						else:
							row.append('-')
				mywriter.writerow(row)
			mywriter.writerow([' '])

def numUniqueHashes(x):
	"""
	Desc:

	Args:

	Returns:

	"""
	y = np.unique(x, axis=0)
	return y.shape[0]

def getShannonEntropy(x):
	"""
	Desc:

	Args:

	Returns:

	"""
	raise NotImplementedError


def getWeightShapesFromModel(model, library='Keras'):
	"""
	Desc:

	Args:

	Returns:


	"""
	pdb.set_trace()
	weightShapes=[]
	print("Printing From Model")
	if library == 'Keras':
		nLayers = len(model.layers)
		for i in range(nLayers):
			nParamSets = len(model.layers[i].get_weights())
			assert nParamSets%2 == 0
			for j in range(int(nParamSets/2)):
				weightShapes.append([model.layers[i].get_weights()[2*j].shape, model.layers[i].get_weights()[2*j+1].shape])
				print(weightShapes[-1])
	return weightShapes


def getWeightShapesFromH5(fileName):
	"""
	Desc:

	Args:

	Returns:

	"""
	f = h5py.File(fileName, 'r')
	allKeys=[k for k in f.keys()]
	weightShapes=[]
	print("Printing From saved weights")
	for layerNumber in range(len(allKeys)):
		subKeys = [k for k in f[allKeys[layerNumber]].keys()]
		assert len(subKeys)%2 == 0
		for i in range(int(len(subKeys)/2)):
			weightShapes.append([f[allKeys[layerNumber]][subKeys[2*i]][:].shape, f[allKeys[layerNumber]][subKeys[2*i+1]][:].shape])
			print(weightShapes[-1])
	f.close()
	return weightShapes


def convertThtoTf(srcFileName, dstFileName):
	"""
	Desc:

	Args:

	Returns:

	"""
	from keras.utils.conv_utils import convert_kernel
	from shutil import copyfile
	copyfile(srcFileName, dstFileName)
	f = h5py.File(dstFileName, 'r+')
	allKeys=[k for k in f.keys()]
	for layerNumber in range(len(allKeys)):
		if 'conv' in allKeys[layerNumber]:
			subKeys = [k for k in f[allKeys[layerNumber]].keys()]
			for i in range(len(subKeys)):
				original_w = f[allKeys[layerNumber]][subKeys[i]][:]
				if i == 0:
					abcd = f[allKeys[layerNumber]][subKeys[0]]
					del f[allKeys[layerNumber]][subKeys[0]]
					original_w = np.transpose(convert_kernel(original_w), (2, 3, 1, 0))
					data = f[allKeys[layerNumber]].create_dataset(subKeys[i], original_w.shape)
					data = original_w
	f.close()


def matToHdf5(srcFileName, dstFileName):
	"""
	"""
	data = io.loadmat(srcFileName)
	datasets = [key for key in data.keys() if '__' not in key]
	f = h5py.File(dstFileName, 'w')
	for i in range(len(datasets)):
		f.create_dataset(datasets[i], data=data[datasets[i]])
	f.close()

def prepImageData(images, chOrder='channelsLast', resizeHeight=256, resizeWidth=256, meanSubtractOrder='BGR'):
	"""
	Desc:

	Args:

	Returns:
	"""
	if chOrder == 'channelsLast':
		images = channelsFirstToLast(images)
	elif chOrder == 'channelsFirst':
		images = channelsLastToFirst(images)
	images = resizedImages(images, resizeHeight, resizeWidth)
	images = meanSubtract(images, meanSubtractOrder)
	return images

def prepLabelData(labels, sourceType='uint', targetType='uint'):
	"""
	Desc:

	Args:

	Returns:
	
	"""
	batchSize = max(labels.shape)
	if sourceType== 'uint' and targetType == 'uint':
		labels = np.reshape(labels, (batchSize, ))
	else:
		raise NotImplementedError


def oneTimePreprocess(srcFileName, dstFileName, processInBatches = False):
	"""
	"""
	fSrc = h5py.File(srcFileName, 'r')
	fDst = h5py.File(dstFileName, 'w')
	datasets = [key for key in fSrc.keys() if '__' not in key]
	for i in range(len(datasets)):
		if len(fSrc[datasets[i]][:].shape) == 4 and np.max(fSrc[datasets[i]][:]) > 200:
			if not processInBatches:
				data = prepImageData(fSrc[datasets[i]][:])
			else:
				raise NotImplementedError
		else:
			data = prepLabelData(fSrc[datasets[i]][:])
		fDst.create_dataset(datasets[i], data=data)
	fSrc.close()
	fDst.close()

def resizeImages(images, resizeHeight=256, resizeWidth = 256):
	"""
	Desc:

	Args:

	Returns:
	"""
	order = images.shape
	ch = order.index(3)
	if ch == 1:
		images = channelsFirstToLast(images)
	resizedImages = np.zeros((images.shape[0], resizeHeight, resizeWidth, 3))
	for i in range(resizedImages.shape[0]):
		resizedImages[i,:,:,:] = resize(images[i], (resizeHeight, resizeWidth))
	if ch == 1:
		resizedImages = channelsLastToFirst(resizedImages)
	return resizedImages


def cropImages(images, cropHeight=227, cropWidth=227):
	"""
	Desc:

	Args:

	Returns:

	"""
	order = images.shape
	ch = order.index(3)
	if ch == 1:
		images = channelsFirstToLast(images)
	croppedImages = np.zeros((images.shape[0], cropHeight, cropWidth, 3))
	for i in range(croppedImages.shape[0]):
		randX = np.random.randint(images.shape[1]-cropHeight)
		randY = np.random.randint(images.shape[2]-cropWidth)
		croppedImages[i,:,:,:] = images[i,randX:randX+cropHeight,randY:randY+cropWidth,:]
	if ch == 1:
		croppedImages = channelsLastToFirst(croppedImages)
	return croppedImages

def channelsFirstToLast(images):
	"""
	Desc:

	Args:

	Returns:

	"""
	order = images.shape
	ch = order.index(3)
	if ch == 1:
		images = np.transpose(images, (0, 2, 3, 1))
	return images

def channelsLastToFirst(images):
	"""
	Desc:

	Args:

	Returns:

	"""
	order = images.shape
	ch = order.index(3)
	if ch == 3:
		images = np.transpose(images, (0, 3, 1, 2))
	return images

def meanSubtract(images, sourceDataSet='IMAGENET', order='RGB'):
	"""
	Desc:

	Args:

	Returns:

	"""
	chOrder = images.shape
	ch = chOrder.index(3)	
	if ch == 1:
		images = channelsFirstToLast(images)
	if order == 'RGB':
		#in RGB order
	    images[:, :, :, 0] -= 123.68
	    images[:, :, :, 1] -= 116.779
	    images[:, :, :, 2] -= 103.939 # values copied from https://github.com/heuritech/convnets-keras/blob/master/convnetskeras/convnets.py
	elif order == 'BGR':
	    images[:, :, :, 0] -= 103.939
	    images[:, :, :, 1] -= 116.779
	    images[:, :, :, 2] -= 123.68 # values copied from https://github.com/heuritech/convnets-keras/blob/master/convnetskeras/convnets.py
	if ch == 1:
		images = channelsLastToFirst(images)
	return images


def shuffleInUnison(images, labels):
	"""
	Desc:

	Args:

	Returns:

	"""
	perm = np.random.permutation(images.shape[0])
	images = images[perm]
	labels = labels[perm]
	return images, labels