import numpy as np
import pdb
from scipy import io
import h5py
# import sys
# sys.path.insert(0, '/media/vrpg/parent/vijetha/CVPR_2018_multiHashing')


def convertLevelDbtoHdf5(sourcePath, targetPath):
	"""
	Description:
		converts a level-db file (generated by caffe) to a hdf5 file.
	Args:
		sourcePath - file path at which the level-db file is present.
		targetPath - file path to which the hdf5 file is to be stored.
	Outputs:
		stores a hdf5file at the give path 'sourcePath'
	"""
	import caffe
	import leveldb
	from caffe.proto import caffe_pb2
	dbData = leveldb.LevelDB(sourcePath)
	datum = caffe_pb2.Datum()

	data = []
	for key, value in dbData.RangeIter():
	    datum.ParseFromString(value)
	    data.append(caffe.io.datum_to_array(datum))
	data = np.array(data)
	pdb.set_trace()
	data = np.reshape(data, (data.shape[0], data.shape[1]))
	f = h5py.File(targetPath, 'w')
	f.create_dataset('data', data=data)
	f.close()


def mergeHdf5Files(listOfFilesToMerge, nameOfNewFile):
	"""
	Desc: 

	Args:

	Outputs:

	"""
	fNew = h5py.File(nameOfNewFile, 'w')
	for i in range(len(listOfFilesToMerge)):
		fOld = h5py.File(listOfFilesToMerge[i][0], 'r')
		data = fOld['data'][:]
		fOld.close()
		fNew.create_dataset(listOfFilesToMerge[i][1], data=data)
		del data
	fNew.close()


def oneHotVectors(x):
	"""
	Desc:

	Args:

	Outputs:

	"""
	if np.max(x) == 0:
		raise AssertionError()
	m = max(x.shape)
	n = np.max(x)
	y = np.zeros((m, n))
	x = np.reshape((m, ))
	y[np.arange(m), x] = 1
	return y

def computemAP(queryHashes, databaseHashes, groundTruthSimilarity):
	"""
	Desc:

	Args:

	Outputs:

	"""
	hammingDist, hammingRank = calcHammingRank(queryHashes, databaseHashes)
	[Q, N] = hammingRank.shape
	pos = np.arange(N)+1
	MAP = 0
	numSucc = 0
	for i in range(Q):
		ngb = groundTruthSimilarity[i, np.asarray(hammingRank[i,:], dtype='int32')]
		nRel = np.sum(ngb)
		if nRel > 0:
			prec = np.divide(np.cumsum(ngb), pos)
			#prec = prec[0:5000]
			#pdb.set_trace()
			#ap = np.mean(prec[np.asarray(ngb[0:5000], dtype='bool')])
			prec = prec
			ap = np.mean(prec[np.asarray(ngb, dtype='bool')])
			MAP = MAP + ap
			numSucc = numSucc + 1
	MAP = float(MAP)/numSucc
	return MAP


def computeSimilarity(queryLabels, databaseLabels, typeOfData='singleLabelled'):
	"""
	Desc:

	Args:

	Outputs:

	"""
	groundTruthSimilarityMatrix = np.zeros((queryLabels.shape[0], databaseLabels.shape[0]))
	if typeOfData=='singleLabelled':
		for i in range(queryLabels.shape[0]):
			groundTruthSimilarityMatrix[i,:] = queryLabels[i] == databaseLabels
	elif typeOfData=='multiLabelled':
		for i in range(queryLabels.shape[0]):
			curQue = queryLabels[i][:]
			if sum(curQue) != 0:
				threshold = 1
				sim = np.sum(np.logical_and(curQue, databaseLabels), axis=-1)
				den = np.sum(np.logical_or(curQue, databaseLabels), axis=-1)
				groundTruthSimilarityMatrix[i][np.where(sim >= threshold)[0]] = 1:
	groundTruthSimilarityMatrix = np.asarray(groundTruthSimilarityMatrix, dtype='float32')
	return groundTruthSimilarityMatrix


def calcHammingRank(queryHashes, databaseHashes):
	"""
	Desc:

	Args:

	Outputs:

	"""
	hammingDist = np.zeros((queryHashes.shape[0], databaseHashes.shape[0]))
	hammingRank = np.zeros((queryHashes.shape[0], databaseHashes.shape[0]))
	for i in range(queryHashes.shape[0]):
		hammingDist[i] = np.reshape(np.sum(np.abs(queryHashes[i] - databaseHashes), axis=1), (databaseHashes.shape[0], ))
		hammingRank[i] = np.argsort(hammingDist[i])
	return hammingDist, hammingRank
